{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --marking_scheme MARKING_SCHEME --image_dir\n",
      "                             IMAGE_DIR --output_dir OUTPUT_DIR\n",
      "ipykernel_launcher.py: error: the following arguments are required: --marking_scheme, --image_dir, --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"MCQ Answer Script Grading Utility\")\n",
    "    \n",
    "    parser.add_argument('--marking_scheme', required=True, help='Path to the marking scheme CSV file')\n",
    "    parser.add_argument('--image_dir', required=True, help='Directory containing answer script images')\n",
    "    parser.add_argument('--output_dir', required=True, help='Directory to store the output reports')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "# Example usage:\n",
    "args = parse_arguments()\n",
    "marking_scheme_path = args.marking_scheme\n",
    "image_dir = args.image_dir\n",
    "output_dir = args.output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [3], [3], [3], [3], [3], [4], [3], [4], [1], [1], [2], [1], [1], [3], [3], [3], [1], [2], [3], [2], [1], [2, 3, 4], [1], [1], [1], [4], [1], [4], [3], [1], [2, 4], [3], [4], [1], [3], [1], [1], [1], [1], [2, 4], [2, 4], [2, 3], [2], [4], [2], [1], [3], [2], [2]] ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'Any', '-', '-', '-', '-', '-', '-', '-', '-', 'All', '-', '-', '-', '-', '-', '-', '-', '-', 'All', 'All', 'All', '-', '-', '-', '-', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_marking_scheme_with_conditions(csv_path):\n",
    "    correct_answers = []\n",
    "    conditions = []\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "        \n",
    "        for row in reader:\n",
    "            # Parse correct answer and condition\n",
    "            correct_answer = row[1]\n",
    "            condition = row[2]\n",
    "            \n",
    "            # Handle multiple correct answers\n",
    "            if ',' in correct_answer:\n",
    "                correct_answer = [int(a) for a in correct_answer.split(',')]\n",
    "            else:\n",
    "                correct_answer = [int(correct_answer)]\n",
    "            \n",
    "            correct_answers.append(correct_answer)\n",
    "            conditions.append(condition)\n",
    "    \n",
    "    return correct_answers, conditions\n",
    "\n",
    "# Example usage:\n",
    "correct_answers, conditions = load_marking_scheme_with_conditions(\"Data Set\\Marking Schemes\\Marking Schemes\\A.csv\")\n",
    "print(correct_answers,conditions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to process image and extract student answers\n",
    "def extract_student_answers(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Apply adaptive thresholding to convert the image to binary\n",
    "    thresh = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    # Detect contours in the binary image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    student_answers = []\n",
    "    # You will need to identify and map contours to questions and answers\n",
    "    # The following is a simplified placeholder assuming the bubbles are found sequentially\n",
    "    # This section must be customized to suit the specific structure of your answer sheet\n",
    "    \n",
    "    # Example: Simulate extracting answers (assuming each contour maps to a question)\n",
    "    for i in range(1, 51):  # Assuming 50 questions\n",
    "        # Simulated extraction of answer (replace with actual processing)\n",
    "        filled_option = np.random.randint(1, 6)  # Randomly assigning 1-5 as an example\n",
    "        student_answers.append(filled_option)\n",
    "\n",
    "    return student_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading function\n",
    "def grade_answers_with_conditions(student_answers, correct_answers, conditions):\n",
    "    score = 0\n",
    "    detailed_results = []\n",
    "    \n",
    "    for student_ans, correct_ans, condition in zip(student_answers, correct_answers, conditions):\n",
    "        if condition == '-' or condition == 'Any':\n",
    "            # Check if the student's answer matches any of the correct answers\n",
    "            if isinstance(correct_ans, list):\n",
    "                if student_ans in correct_ans:\n",
    "                    score += 1\n",
    "                    detailed_results.append(True)  # Correct answer\n",
    "                else:\n",
    "                    detailed_results.append(False)  # Incorrect answer\n",
    "            else:\n",
    "                if student_ans == correct_ans:\n",
    "                    score += 1\n",
    "                    detailed_results.append(True)  # Correct answer\n",
    "                else:\n",
    "                    detailed_results.append(False)  # Incorrect answer\n",
    "\n",
    "        elif condition == 'All':\n",
    "            # Check if the student's answer matches all correct answers\n",
    "            if isinstance(correct_ans, list):\n",
    "                if student_ans == correct_ans:\n",
    "                    score += 1\n",
    "                    detailed_results.append(True)  # Correct answer\n",
    "                else:\n",
    "                    detailed_results.append(False)  # Incorrect answer\n",
    "            else:\n",
    "                if student_ans == correct_ans:\n",
    "                    score += 1\n",
    "                    detailed_results.append(True)  # Correct answer\n",
    "                else:\n",
    "                    detailed_results.append(False)  # Incorrect answer\n",
    "\n",
    "    return score, detailed_results\n",
    "\n",
    "# Example usage:\n",
    "#score, detailed_results = grade_answers_with_conditions(\"Data Set\\Answer Scripts\\Answer Scripts\\JJ5.jpg\", correct_answers, conditions)\n",
    "\n",
    "#print(score, detailed_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute the whole process\n",
    "def grade_image(image_path, marking_scheme_path):\n",
    "    # Load correct answers and conditions\n",
    "    correct_answers, conditions = load_marking_scheme_with_conditions(marking_scheme_path)\n",
    "\n",
    "    # Extract student answers from image\n",
    "    student_answers = extract_student_answers(image_path)\n",
    "\n",
    "    # Grade the student answers\n",
    "    score, detailed_results = grade_answers_with_conditions(student_answers, correct_answers, conditions)\n",
    "\n",
    "    return score, detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 9\n",
      "Details: [False, False, False, False, False, False, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image_path = \"Data Set\\Answer Scripts\\Answer Scripts\\JJ5.jpg\"\n",
    "marking_scheme_path = \"Data Set\\Marking Schemes\\Marking Schemes\\A.csv\"\n",
    "\n",
    "score, detailed_results = grade_image(image_path, marking_scheme_path)\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"Details: {detailed_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
